<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>R on Blog</title><link>https://jaronoff97.github.io/tags/r/</link><description>Recent content in R on Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 16 Apr 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://jaronoff97.github.io/tags/r/index.xml" rel="self" type="application/rss+xml"/><item><title>DS 4100 Week 13 Review</title><link>https://jaronoff97.github.io/archives/2017-04-16-ds-4100-week-13-review/</link><pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-04-16-ds-4100-week-13-review/</guid><description>DS 4100 Weekly Review This is going to be my last weekly blog post, so I&amp;rsquo;m going to take this oppurtunity to reflect on this year. Before I do that, a quick project update:
I decided this weekend that I&amp;rsquo;m going to have to spin up an AWS cluster in order to connect to google BigQuery. Though it was not my first choice, I&amp;rsquo;m pretty sure it&amp;rsquo;s the only way this project is going to work.</description></item><item><title>DS 4100 Week 12 Review</title><link>https://jaronoff97.github.io/archives/2017-04-09-ds-4100-week-12-review/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-04-09-ds-4100-week-12-review/</guid><description>DS 4100 Weekly Review This weekend I worked on the extra credit assignments and did some more research on my project. I&amp;rsquo;ll start this post with what I learned when doing research with my project and then walkthrough how my extra credit assignments went.
Research project So this weekend, I was looking a lot into the publicly available data sets for reddit. I found a HUGE one available on google BigQuery which I&amp;rsquo;ve never used before.</description></item><item><title>DS 4100 Day 21</title><link>https://jaronoff97.github.io/archives/2017-04-04-ds-4100-day-21/</link><pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-04-04-ds-4100-day-21/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re doing coding walkthroughs, nothing too exciting.</description></item><item><title>DS 4100 Week 11 Review</title><link>https://jaronoff97.github.io/archives/2017-04-02-ds-4100-week-11-review/</link><pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-04-02-ds-4100-week-11-review/</guid><description>DS 4100 Weekly Review This week I begin my project for my data science class. As my project proposal says, I&amp;rsquo;m going to be doing collaborative filtering on a large reddit data set to determine other communities a user may like. I think this could be a really fun project that I&amp;rsquo;d be excited to share with the reddit community. I&amp;rsquo;m excited to get started on the project, and I think i&amp;rsquo;ll begin working on it this week.</description></item><item><title>DS 4100 Project Proposal</title><link>https://jaronoff97.github.io/archives/2017-03-30-ds-4100-project-proposal/</link><pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-30-ds-4100-project-proposal/</guid><description>DS 4100 Project Proposal For my project, I want to investigate Reddit. There&amp;rsquo;s a lot of openly available data about reddit that I&amp;rsquo;ll be able to scrape. Recently, there was a brief study posted on FiveThirtyEight doing what he called &amp;ldquo;Reddit Algebra&amp;rdquo;, which was actually just a fancier way of explaining set operations.
FiveThirtyEight Article
For the past couple of weeks in my weekly reviews, I&amp;rsquo;ve talked a lot about analyzing reddit data to generate predictions or statistics.</description></item><item><title>DS 4100 Week 10 Review</title><link>https://jaronoff97.github.io/archives/2017-03-26-ds-4100-week-10-review/</link><pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-26-ds-4100-week-10-review/</guid><description>DS 4100 Weekly Review This week I want to talk about one of the most interesting stories i&amp;rsquo;ve seen in the past couple of months. The story was on FiveThirtyEight, it was about analyzing one of the most controversial subreddits on reddit: The_Donald. The author ran a bunch of queries about reddit to insert data into a huge database. He then used this information to do what the author coined as &amp;ldquo;Reddit Algebra&amp;rdquo;.</description></item><item><title>DS 4100 Day 17</title><link>https://jaronoff97.github.io/archives/2017-03-21-ds-4100-day-17/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-21-ds-4100-day-17/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re talking more about predictive analytics
Prediction Confidence Forecast range A forecast should be given as a range. The range to provide is the 95% Confidence Interval, i.e., the range into which there is a 95% probability that the actual value will fall. Forecasting models must be continually evaluated to assure that they still provide accurate forecast estimates. The tracking signal (TS) is a measure of the quality of the forecasts:</description></item><item><title>DS 4100 Week 9 Review</title><link>https://jaronoff97.github.io/archives/2017-03-19-ds-4100-week-9-review/</link><pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-19-ds-4100-week-9-review/</guid><description>DS 4100 Weekly Review This week I want to brainstorm more about my project. I really like the idea of doing some reddit scraping. I&amp;rsquo;ve been looking at a lot of cool reddit data science projects. There have been a couple stand out projects in the past year that have really interested me; there was one about the occurance of swear words in different popular subreddits; another was about linking to news websites in different political subreddits; another was about subreddits that are the most supportive/ kind.</description></item><item><title>DS 4100 Day 16</title><link>https://jaronoff97.github.io/archives/2017-03-17-ds-4100-day-16/</link><pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-17-ds-4100-day-16/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re doing more on regression model predictions.
Regression Regression models are a mathematical equation used to predict a value based on empirical observations. The prediction is never correct, but, depending on the “fit of data,” it can be reasonably good.
The variable to be predicted is called the dependent variable
Sometimes also called the response variable The value of this variable depends on the value(s) of the independent variable(s)</description></item><item><title>DS 4100 Week 8 Review</title><link>https://jaronoff97.github.io/archives/2017-03-05-ds-4100-week-8-review/</link><pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-03-05-ds-4100-week-8-review/</guid><description>DS 4100 Weekly Review This week we focused more on NoSQL, I was happy to learn more about different ways to store data. I was also excited to learn more about the different kinds of NoSQL store management such as key-value, graph, columnar, and document. I&amp;rsquo;m very interested in data pipelines, and I think in my project I&amp;rsquo;m going to use a graph storage system to represent my data. Either that or a document store.</description></item><item><title>DS 4100 Day 15</title><link>https://jaronoff97.github.io/archives/2017-02-28-ds-4100-day-15/</link><pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-28-ds-4100-day-15/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re continuing our lesson on statistics.
Statistical Inference Statistical inference combines the methods of descriptive statistics with the theory of probability to infer characteristics of a large population from small sample. The sample must be selected randomly and must be “large enough” to be statistically significant.
Statistical Significance Statistical significance means that the characteristics of the sample are likely not due to chance or random error.</description></item><item><title>DS 4100 Week 7 Review</title><link>https://jaronoff97.github.io/archives/2017-02-26-ds-4100-week-7-review/</link><pubDate>Sun, 26 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-26-ds-4100-week-7-review/</guid><description>DS 4100 Weekly Review This week has been pretty relaxed, I enjoyed getting to work more and more on databases. It&amp;rsquo;s really fun to script for databases. R is not usually enjoyable, but actually I really liked the ability to quickly do this assignment. It turns out that R is good at doing these kinds of quick scripts where you dont have to care about types. When I was programming this time around it felt way more like programming in Python, which obviously, I enjoy a lot.</description></item><item><title>DS 4100 Day 14</title><link>https://jaronoff97.github.io/archives/2017-02-24-ds-4100-day-14/</link><pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-24-ds-4100-day-14/</guid><description>DS 4100 Data Collection, Integration, and Analysis Descriptive Analytics When analyzing data it is critical to ask where the data comes from and how it was produced, obtained, or collected.
In particular, the quality of the data must be assessed and, if necessary, the analysis must be qualified if data is of poor quality or partially missing.
Data provenance describes the organizational processes that are in place to ensure accurate collection and curation of the data.</description></item><item><title>DS 4100 Day 13</title><link>https://jaronoff97.github.io/archives/2017-02-21-ds-4100-day-13/</link><pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-21-ds-4100-day-13/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re talking a bit about NoSQL databases. These databases can be divided into four generes:
Key-Value Columnar Document Graph Key-Value Key-Value (KV) databases such as Redis and Riak are the simplest kind of database in which keys are stored with matching values. A KV-DB is essentially a lookup table that often uses hashing to speed up retrieval. KV-DBs scale easily and have high performance.</description></item><item><title>DS 4100 Week 6 Review</title><link>https://jaronoff97.github.io/archives/2017-02-19-ds-4100-week-6-review/</link><pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-19-ds-4100-week-6-review/</guid><description>DS 4100 Weekly Review This week I was very excited to update my scraper for more functionality. Beyond that I focused more on turning this into a full on project. I&amp;rsquo;m looking into more permanent solutions for what im trying to do. Definetly when I start my project I&amp;rsquo;ll need to have this basic stack:
RedditAPI for python Dedicated AWS/ DigitalOcean hosting Probably a relational database for storage Redis for key value caching React front end with d3 for the vis Maybe I&amp;rsquo;ll rope in someone else to help with my project.</description></item><item><title>DS 4100 Day 12</title><link>https://jaronoff97.github.io/archives/2017-02-17-ds-4100-day-12/</link><pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-17-ds-4100-day-12/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re talking more about SQL.
Three primary operations in relational database:
Selection retrieve specific rows from a table Projection retrieve specific columns from a table Join combine multiple tables The basic schema for a query is as follows:
SELECT &amp;lt;attribute list&amp;gt; FROM &amp;lt;table list&amp;gt; WHERE &amp;lt;condition&amp;gt; GROUP BY &amp;lt;column&amp;gt; HAVING &amp;lt;search-condition&amp;gt; ORDER BY &amp;lt;column&amp;gt;; Column Calculation In SQL it&amp;rsquo;s possible to calculate columns on the fly:</description></item><item><title>DS 4100 Day 11</title><link>https://jaronoff97.github.io/archives/2017-02-14-ds-4100-day-11/</link><pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-14-ds-4100-day-11/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re talking about storing data in a relational database (MySQL). We&amp;rsquo;re going to be focusing on SQL right now, in a couple of weeks we&amp;rsquo;re going to be doing work in NoSQL; MongoDB, couch, etc. Eventually, I&amp;rsquo;ll have to make a full pipeline of data, starting with collection, then storage, then retrieval, and finally a predictive model. Finally, there needs to be a report; possibly a visualization.</description></item><item><title>DS 4100 Week 5 Review</title><link>https://jaronoff97.github.io/archives/2017-02-12-ds-4100-week-5-review/</link><pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-12-ds-4100-week-5-review/</guid><description>DS 4100 Weekly Review This week I had to write a scraper for class. This was a fun assignment that I got to do in Python. I had a couple of ideas for this assignment, but I was able to decide upon scraping reddit. Reddit has an api, however, this assignment specified that we scrape HTML. I&amp;rsquo;m really interested in eventually doing sentiment analysis; how does what someone says affect the amount of points they get?</description></item><item><title>DS 4100 Day 10</title><link>https://jaronoff97.github.io/archives/2017-02-10-ds-4100-day-10/</link><pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-10-ds-4100-day-10/</guid><description>DS 4100 Data Collection, Integration, and Analysis The reason some of the previous days are missing is because they were either work days or we didn&amp;rsquo;t have class. Today we&amp;rsquo;re talking about scraping.
Scraping Scraping often starts with making GET requests. Take the HTML and search through the HTML for the data.
Process:
Get a website -&amp;gt; Scrape it -&amp;gt; Get data
Be careful of websites that do not allow web scraping.</description></item><item><title>DS 4100 Week 4 Review</title><link>https://jaronoff97.github.io/archives/2017-02-05-ds-4100-week-4-review/</link><pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-02-05-ds-4100-week-4-review/</guid><description>DS 4100 Weekly Review WOW! What a great game tonight! I&amp;rsquo;m really interested in the statistics and probability behind the game. I was looking into what sort of data I can get online and actually found a couple data sources. According to one article I found, the ball was only in play for a total of 16 minutes and 14 seconds. Link to article
What I was more interested in, however, was the probability of the game.</description></item><item><title>DS 4100 Day 7</title><link>https://jaronoff97.github.io/archives/2017-01-31-ds-4100-day-7/</link><pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-31-ds-4100-day-7/</guid><description>DS 4100 Data Collection, Integration, and Analysis Today we&amp;rsquo;re talking about how to use excel to check the integrity of your results. It&amp;rsquo;s all about table lookup. Searches in excel can only return a single value.
Excel is boring and annoying and whenever I need to work in it, I&amp;rsquo;m either going to run SQL queries or write some python code to do whatever I want. We&amp;rsquo;ve been talking a lot about it, but I&amp;rsquo;m zoning out to it.</description></item><item><title>DS 4100 Week 3 Review</title><link>https://jaronoff97.github.io/archives/2017-01-29-ds-4100-week-3-review/</link><pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-29-ds-4100-week-3-review/</guid><description>DS 4100 Weekly Review Very exicted for next week! This past week was pretty fun, I enjoyed getting the chance to use R&amp;rsquo;s interesting type system. Our assignment this week was to basically convert string -&amp;gt; datetime -&amp;gt; int -&amp;gt; datetime. I did this in a pretty interesting way. First, I made a new column in the data frame which represented the &amp;ldquo;Posix&amp;rdquo; version of the string in the &amp;ldquo;date&amp;rdquo; column.</description></item><item><title>DS 4100 Day 6</title><link>https://jaronoff97.github.io/archives/2017-01-27-ds-4100-day-6/</link><pubDate>Fri, 27 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-27-ds-4100-day-6/</guid><description>DS 4100 Data Collection, Integration, and Analysis A lot of assignments coming up so I may blog about that but TBD.
Today was a pretty insignificant day: we went over our future assignments, and then did a show and tell about our first assignment to see how many different ways there are to complete it. I anticipate there&amp;rsquo;s going to be a longer post on Monday.</description></item><item><title>DS 4100 Day 5</title><link>https://jaronoff97.github.io/archives/2017-01-24-ds-4100-day-5/</link><pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-24-ds-4100-day-5/</guid><description>DS 4100 Data Collection, Integration, and Analysis Like my OOD posts, I wasn&amp;rsquo;t here in class for day 4, so I&amp;rsquo;m skipping right to day 5. Today/ this week we&amp;rsquo;re doing data importing from multiple sources as well as scraping. We started by importing CSVs and txt files. Now we&amp;rsquo;re going to import from other normal files, JSON, and XML. Eventually we&amp;rsquo;re going to be getting data from databases.
Packages Packages are collections of R functions, data, and compield code in a well-defined format.</description></item><item><title>DS 4100 Week 2 Review</title><link>https://jaronoff97.github.io/archives/2017-01-22-ds-4100-week-2-review/</link><pubDate>Sun, 22 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-22-ds-4100-week-2-review/</guid><description>DS 4100 Weekly Review Continuing my journey from last week, I was excited to begin this week devling more into the R programming language. It&amp;rsquo;s interesting and aggravating to learn about R. I talked with some of my friends about the strengths and (many) weeknesses of the language this week at NU Hacks. Many commented on how the type system allowed users to make many errors that could be easily prevented.</description></item><item><title>DS 4100 Day 3</title><link>https://jaronoff97.github.io/archives/2017-01-17-ds-4100-day-3/</link><pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-17-ds-4100-day-3/</guid><description>DS 4100 Data Collection, Integration, and Analysis Day 3, we&amp;rsquo;re going over basic control flow:
&amp;gt; for (i in 1:3) { print(paste(&amp;#34;i =&amp;#34;,i)) } [1] &amp;#34;i = 1&amp;#34; [1] &amp;#34;i = 2&amp;#34; [1] &amp;#34;i = 3&amp;#34; &amp;gt; i [1] 3 cities &amp;lt;- c(&amp;#34;Boston&amp;#34;, &amp;#34;New York&amp;#34;, &amp;#34;San Francisco&amp;#34;) for(city in cities) { print(city) } You can also use for loops to iterate data frames.
# create a data frame c1&amp;lt;-c(&amp;#34;AA&amp;#34;,&amp;#34;BB&amp;#34;,&amp;#34;CC&amp;#34;) c2&amp;lt;-c(11,22,33) df&amp;lt;-data.</description></item><item><title>DS 4100 Week 1 Review</title><link>https://jaronoff97.github.io/archives/2017-01-15-ds-4100-week-1-review/</link><pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-15-ds-4100-week-1-review/</guid><description>DS 4100 Weekly Review This week I began my journey into data science. It&amp;rsquo;s the first time I&amp;rsquo;ve taken a class like this, and so far I&amp;rsquo;m pretty excited. The class began with an introduction to our syllabus and other things we&amp;rsquo;re going to be doing in and out of class, including these blog reflections. In these reflections I&amp;rsquo;m going to be thinking about what I learned in class, and connecting it to what I&amp;rsquo;ve experienced in the world.</description></item><item><title>DS 4100 Day 2</title><link>https://jaronoff97.github.io/archives/2017-01-13-ds-4100-day-2/</link><pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-13-ds-4100-day-2/</guid><description>DS 4100 Data Collection, Integration, and Analysis Everyone is downloading R, meanwhile I&amp;rsquo;m just sitting here and finishing up Java.
Data is stored as objects in R. Objects are created by:
Reading data from an external file Retrieving data from a URL Creating an object directly from command line Instantiating an object from within a program Now we&amp;rsquo;re going over R, basically what I learned in the past (6) tutorials.</description></item><item><title>Learning R Part 6</title><link>https://jaronoff97.github.io/archives/2017-01-11-learning-r-part-6/</link><pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-11-learning-r-part-6/</guid><description>Learning R Part 6 Part 6. Oh god. This language. Let it be over. Well this is working with enviroments which will be useful for my data science classes. I&amp;rsquo;m gonna dive right in.
&amp;gt; library(pryr) &amp;gt; parenvs(all = True) Error: object &amp;#39;True&amp;#39; not found &amp;gt; parenvs(all = T) label name 1 &amp;lt;environment: R_GlobalEnv&amp;gt; &amp;#34;&amp;#34; 2 &amp;lt;environment: package:pryr&amp;gt; &amp;#34;package:pryr&amp;#34; 3 &amp;lt;environment: package:stats&amp;gt; &amp;#34;package:stats&amp;#34; 4 &amp;lt;environment: package:graphics&amp;gt; &amp;#34;package:graphics&amp;#34; 5 &amp;lt;environment: package:grDevices&amp;gt; &amp;#34;package:grDevices&amp;#34; 6 &amp;lt;environment: package:utils&amp;gt; &amp;#34;package:utils&amp;#34; 7 &amp;lt;environment: package:datasets&amp;gt; &amp;#34;package:datasets&amp;#34; 8 &amp;lt;environment: package:methods&amp;gt; &amp;#34;package:methods&amp;#34; 9 &amp;lt;environment: 0x7f9a81097d78&amp;gt; &amp;#34;Autoloads&amp;#34; 10 &amp;lt;environment: base&amp;gt; &amp;#34;&amp;#34; 11 &amp;lt;environment: R_EmptyEnv&amp;gt; &amp;#34;&amp;#34; &amp;gt; as.</description></item><item><title>DS 4100 Day 1</title><link>https://jaronoff97.github.io/archives/2017-01-10-ds-4100-day-1/</link><pubDate>Tue, 10 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-10-ds-4100-day-1/</guid><description>DS 4100 Data Collection, Integration, and Analysis Course Website
Just like my CS 3500 posts, this series is for my data science class notes. This class is what most of my R tutorial posts have been for (I&amp;rsquo;m going to continue those until I finish the book). Here are the units:
Unit 1 - Essentials Concepts of Data Science Unit 2 - Programming in R for Data Science Unit 3 - Data Collection &amp;amp; Integration Unit 4 - Data Storage Unit 5 - Data Analytics Unit 6 - Python Programming for Data Science Unit 7 - Data Quality &amp;amp; Governance So it seems like we start with R, and eventually get to Python.</description></item><item><title>Learning R Part 5</title><link>https://jaronoff97.github.io/archives/2017-01-08-learning-r-part-5/</link><pubDate>Sun, 08 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-08-learning-r-part-5/</guid><description>Learning R Part 5 Part 5, modifying values in R. I&amp;rsquo;m betting this is going to be a contiuation of the same. It seems we&amp;rsquo;re just going to combine setting values and indexing.
&amp;gt; vec &amp;lt;- c(0, 0, 0, 0, 0, 0) &amp;gt; vec [1] 0 0 0 0 0 0 &amp;gt; vec[1] [1] 0 &amp;gt; vec[1] &amp;lt;- 1000 &amp;gt; vec [1] 1000 0 0 0 0 0 &amp;gt; vec[c(1, 3, 5)] &amp;lt;- c(1, 1, 1) &amp;gt; vec [1] 1 0 1 0 1 0 &amp;gt; vec[4:6] &amp;lt;- vec[4:6] + 1 &amp;gt; vec [1] 1 0 1 1 2 1 &amp;gt; deck &amp;lt;- read.</description></item><item><title>Learning R Part 4</title><link>https://jaronoff97.github.io/archives/2017-01-07-learning-r-part-4/</link><pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-07-learning-r-part-4/</guid><description>Learning R Part 4 Part 4, R notation. This part is about shuffling, dealing, and accesing cards. Okay so, the first part is selecting values. Before anything I need to load the deck again.
&amp;gt; deck[ , ] (ALL THE CARDS) &amp;gt; deck[1,1] [1] king Levels: ace eight five four jack king nine queen seven six ten three two &amp;gt; deck[1, 1] [1] king Levels: ace eight five four jack king nine queen seven six ten three two &amp;gt; deck[1, c(1, 2, 3)] face suit value 1 king spades 13 &amp;gt; deck[1, 1:3] face suit value 1 king spades 13 &amp;gt; deck[c(1, 1), 1:3] face suit value 1 king spades 13 1.</description></item><item><title>Learning R Part 3</title><link>https://jaronoff97.github.io/archives/2017-01-06-learning-r-part-3/</link><pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-06-learning-r-part-3/</guid><description>Learning R Part 3 Part 3, this chapter is all about R objects and I&amp;rsquo;m not excited. R objects, from what I&amp;rsquo;ve gathered are barely objects. We start this chapter with a bunch of different data types. R has six basic atomic vector data types: doubles, integers, characters, logicals, complex, and raw.
&amp;gt; die &amp;lt;- c(1, 2, 3, 4, 5, 6) &amp;gt; die [1] 1 2 3 4 5 6 &amp;gt; is.</description></item><item><title>Learning R Part 2</title><link>https://jaronoff97.github.io/archives/2017-01-05-learning-r-part-2/</link><pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-05-learning-r-part-2/</guid><description>Learning R Part 2 Part 2, here we go! This next part is about packages and help pages. This chapter is pretty short, and basically wraps up the dice &amp;lsquo;project&amp;rsquo;. Now it&amp;rsquo;s time to install the qplot function which is part of the ggplot2 package.
&amp;gt; install.packages(&amp;#34;ggplot2&amp;#34;) (A BUNCH OF STUFF!!!!!) &amp;gt; qplot Error: object &amp;#39;qplot&amp;#39; not found &amp;gt; library(&amp;#34;ggplot2&amp;#34;) &amp;gt; qplot function (x, y = NULL, ..., data, facets = NULL, margins = FALSE, geom = &amp;#34;auto&amp;#34;, xlim = c(NA, NA), ylim = c(NA, NA), log = &amp;#34;&amp;#34;, main = NULL, xlab = deparse(substitute(x)), ylab = deparse(substitute(y)), asp = NA, stat = NULL, position = NULL) { if (!</description></item><item><title>Learning R Part 1</title><link>https://jaronoff97.github.io/archives/2017-01-04-learning-r-part-1/</link><pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate><guid>https://jaronoff97.github.io/archives/2017-01-04-learning-r-part-1/</guid><description>Learning R Part 1 Okay so today I begin learning R for my data science class, I&amp;rsquo;m gonna try and make a post a day. From what a lot of my friends have told me R is a pretty crappy language, I&amp;rsquo;m excited to see what the language holds.
I&amp;rsquo;m going to be following the Hands-On Programming with R
To follow these tutorials, I&amp;rsquo;m going to be using the R repl (I installed a package in Sublime Text to handle all of that).</description></item></channel></rss>