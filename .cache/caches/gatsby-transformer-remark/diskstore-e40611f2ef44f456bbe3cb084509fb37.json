{"expireTime":9007200815377754000,"key":"transformer-remark-markdown-html-f2aea990ca1e52db7106b00aa6a566d2-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-autolink-headersgatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypants-","val":"<h3 id=\"ds-4100-data-collection-integration-and-analysis\"><a href=\"#ds-4100-data-collection-integration-and-analysis\" aria-label=\"ds 4100 data collection integration and analysis permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DS 4100 Data Collection, Integration, and Analysis</h3>\n<p>The reason some of the previous days are missing is because they were either work days or we didn’t have class. Today we’re talking about scraping. </p>\n<h2 id=\"scraping\"><a href=\"#scraping\" aria-label=\"scraping permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scraping</h2>\n<p>Scraping often starts with making GET requests. Take the HTML and search through the HTML for the data.</p>\n<p>Process:</p>\n<p>Get a website -> Scrape it -> Get data</p>\n<p>Be careful of websites that do not allow web scraping. A LOT of data is copyrighted. Regex is very useful when writing a scraper. </p>\n<p>Now we’re going over how to scrape in R, which is a weird process. I wrote a scraper in python, and I’m just going to translate it into R, it’s going to be weird, but it should work. I think I’m going to use the <strong>rvest</strong> package, it seeems easy to use and it’s supposed to be inspired by <strong>BeautifulSoup</strong> which is the package I was using in Python.</p>"}